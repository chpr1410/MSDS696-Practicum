{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. Import Packages\n",
    "\n",
    "2. Define Universe of Stocks and Proxies\n",
    "\n",
    "3. Downloading, Combining, and Cleaning the Data\n",
    "    - 3.1 Data Downloader\n",
    "    - 3.2 Data Combiner\n",
    "    - 3.3 Data Cleaner\n",
    "    - 3.4 Feature Engineering First Pass\n",
    "\n",
    "4. Feature Engineering\n",
    "\n",
    "5. Data Preparation\n",
    "    - 5.1 Data Stationarity Check\n",
    "    - 5.2 Train Test Split and Feature Scaling\n",
    "    - 5.3 Split Data into Days\n",
    "    - 5.4 Split Daily Data into Trading Windows\n",
    "    \n",
    "6. Data Pipelining\n",
    "\n",
    "7. Model Construction\n",
    "\n",
    "8. Fine Tuning the Models\n",
    "\n",
    "9. Running the Models for all ETFs\n",
    "\n",
    "10. Resutls Summary/Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from pymongo import MongoClient,InsertOne\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import models, layers, backend, regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "######### Alphavantage key\n",
    "#Chris_for_prime@outlook.com\n",
    "key = 'Z8JAUR4XUY8O1CV2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Define Universe of Stocks and Proxies\n",
    "\n",
    "For this project I wanted to analyze Exchange Traded Funds (ETFs) and use features that serve as proxies for economic inidcators.\n",
    "\n",
    "List of ETFs: https://etfdb.com/compare/volume/\n",
    "   - High volume now and 2 years ago.  My definition of high volume is > 100,000 shares per day.  Most are much higher than that.\n",
    "\n",
    "Economic Indicators:\n",
    "   - Economic Growth\n",
    "   - Inflation\n",
    "   - Unemployment\n",
    "   - Business Confidence\n",
    "   - Housing\n",
    "    \n",
    "Proxies for Each Economic Indicator:\n",
    "   - SPDR S&P 500 ETF Trust (SPY)\n",
    "   - iShares 20+ Year Treasury Bond ETF (TLT)\n",
    "   - Barclays iPath Series B S&P 500 VIX Short-Term Futures ETN (VXX)\n",
    "   - Consumer Discretionary Select Sector SPDR Fund (XLY)\n",
    "   - Vanguard Real Estate Index Fund (VNQ)\n",
    "\n",
    "Market Holidays\n",
    "   - Load in list of market holidays.  The stock market is closed on these days (either partially or completely) and so the data will not be included in training the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    SPY\n",
       "1    TLT\n",
       "2    VXX\n",
       "3    XLY\n",
       "4    VNQ\n",
       "Name: Symbol, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Stocks\n",
    "CoList = pd.read_excel('Input Files/List of ETFs.xlsx',sheet_name='Low_Missing')\n",
    "CoList = CoList['Symbol']\n",
    "\n",
    "# List of Proxies\n",
    "proxies = pd.read_excel('Input Files/List of ETFs.xlsx',sheet_name='Proxies')\n",
    "proxies = proxies['Symbol']\n",
    "\n",
    "# List of Market Holidays\n",
    "holiday_list = list(pd.read_excel(\"Input Files/Stock Market Holidays.xlsx\")['Date'])\n",
    "holiday_list = [holiday.strftime('%Y-%m-%d') for holiday in holiday_list]\n",
    "\n",
    "################## Set Interval #######################\n",
    "# 1min, 5min, 15min, 30min, 60min\n",
    "minute_interval = '1min'\n",
    "\n",
    "proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Downloading, Combining, and Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data Downloader\n",
    "\n",
    "Downloads data from AlphaVantage using API Calls.\n",
    "Data is downloaded in CSV files, with each monthy being one file.\n",
    "Files are named individually and stored in folders named for each ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 93.83it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_downloader(minute_interval, co):\n",
    "    #################### Set Time Periods to DL ##########################\n",
    "    years = [1,2]\n",
    "    months = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "    ################# Header ###################################\n",
    "\n",
    "    ts = TimeSeries(key, output_format='csv')\n",
    "\n",
    "\n",
    "    #################### Check if Data Exists, if not, Download #######################\n",
    "    # See if folder exists, if not, create it\n",
    "    destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '1. Raw Data' / co\n",
    "    if not destination.exists():\n",
    "        destination.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    limit_reached = False\n",
    "\n",
    "    # For each month, download data if needed\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            \n",
    "            # If API limit is not reached\n",
    "            if limit_reached == False:\n",
    "\n",
    "                file_name = minute_interval +co+' - year'+str(year)+'month'+str(month)+'.csv'\n",
    "                file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval/ '1. Raw Data' / co / file_name\n",
    "\n",
    "                if not file.is_file():\n",
    "                    print(co, year, month)\n",
    "                    data, meta_data = ts.get_intraday_extended(symbol=co,interval=minute_interval, slice='year'+str(year)+'month'+ str(month))\n",
    "\n",
    "                    df_list = []\n",
    "                    for row in data:\n",
    "                        df_list.append(row)\n",
    "                    df = pd. DataFrame(df_list)\n",
    "                    df.columns = df.iloc[0]\n",
    "                    df = df.drop(0)\n",
    "                    df = df.reset_index(drop=True)\n",
    "\n",
    "                    # If df has content and not the error message, save it\n",
    "                    if len(df) > 2:\n",
    "                        # Save the DF\n",
    "                        df.to_csv(file,index=False)\n",
    "                        \n",
    "                        sleep(12)\n",
    "\n",
    "                    else:\n",
    "                        print(\"API Limit Reached\")\n",
    "                        limit_reached = True\n",
    "                        break  \n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "    return\n",
    "\n",
    "#### Download ETF and Proxy Data\n",
    "for co in tqdm(CoList):\n",
    "    #print(co)\n",
    "    data_downloader(minute_interval, co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Combiner\n",
    "\n",
    "Combines the monthly pricing data into one CSV.  The AlphaVantage API returns a CSV for each month of data.  For two years of data this is 24 separate CSVs.  This function combines them into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 717.87it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_combiner(minute_interval, co):\n",
    "\n",
    "    # Set Filename\n",
    "    file_name = minute_interval + \" \" + co + ' combined_raw.csv'\n",
    "    file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '2. Combined Raw Data' / file_name \n",
    "        \n",
    "    if not file.is_file():\n",
    "    \n",
    "        col_names = ['time','open','high','low','close','volume']\n",
    "\n",
    "        # Initialize empty DF\n",
    "        main_df = pd.DataFrame(columns=col_names)\n",
    "\n",
    "        # Get Files for each company\n",
    "        destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '1. Raw Data' / co\n",
    "        p = destination.glob('**/*')\n",
    "        files = [x for x in p if x.is_file()]\n",
    "\n",
    "        # Loop through files and combine into one big DF\n",
    "        for file in files:\n",
    "            test_df = pd.read_csv(file)\n",
    "            main_df = pd.concat([main_df,test_df],ignore_index=True)\n",
    "\n",
    "            # Sort by time\n",
    "            main_df.sort_values(\"time\", inplace = True, ignore_index=True) \n",
    "            main_df.drop_duplicates(subset =\"time\", keep = 'first', inplace = True, ignore_index=True)\n",
    "\n",
    "        # See if folder exists, if not, create it\n",
    "        destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '2. Combined Raw Data'\n",
    "        if not destination.exists():\n",
    "            destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save File\n",
    "        main_df.to_csv(file, index=False)\n",
    "\n",
    "    else:\n",
    "        return \n",
    "    \n",
    "    return\n",
    "\n",
    "#### Combine ETF Data\n",
    "for co in tqdm(CoList):\n",
    "    #print(\"Combining\",co)\n",
    "    data_combiner(minute_interval, co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Cleaner\n",
    "\n",
    "- Trims the combined datasets to only include minutes that occur during market hours.  \n",
    "- Adds columns for datetime and hour/minute.\n",
    "- Makes sure all minutes during the day are represented by a row, and for the missing rows, impute values from prior minute.\n",
    "- Keep log of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 887.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_cleaner(minute_interval, co):\n",
    "    \n",
    "    # Set Filename\n",
    "    file_name = minute_interval + \" \" + co + ' cleaned_combined.csv'\n",
    "    file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '3. Cleaned Combined Data' / file_name \n",
    "\n",
    "    if not file.is_file():\n",
    "    \n",
    "        print(\"++++++++++++++++++++++++++++\")\n",
    "        print(f\"Cleaning for: {co}\")\n",
    "\n",
    "        file_name = minute_interval + \" \" + co + ' combined_raw.csv'\n",
    "        file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '2. Combined Raw Data' / file_name \n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        ################################ Add columns for date data ##########################################################\n",
    "\n",
    "        # Create datetime from given string date\n",
    "\n",
    "        df['datetime'] = pd.to_datetime(df['time'])\n",
    "\n",
    "        # Create short date\n",
    "        df['short_date'] = df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "        # Create hour/minute feature\n",
    "        df['hour_minute'] = df['datetime'].dt.strftime('%H:%M:%S')\n",
    "        # Create Hour Feature\n",
    "        df['hour'] = df['datetime'].dt.hour\n",
    "        # Create minute feature\n",
    "        df['minute'] = df['datetime'].dt.minute\n",
    "        # Day of the week integer\n",
    "        df['weekday'] = df['datetime'].dt.weekday \n",
    "\n",
    "        ############################### Data Cleaning ####################################################################\n",
    "\n",
    "        missing_percents_list = []\n",
    "        dates_list = []\n",
    "        clean_combined = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "        print(\"Walking through the days\")\n",
    "        # Group the whole combined df by date\n",
    "        for doc in df.groupby(df['short_date']):\n",
    "\n",
    "            # For each day, if it's not a holiday, clean the day, log the missing values, impute missing values, and combine back into the main DF    \n",
    "            if doc[0] not in holiday_list:\n",
    "                sample = doc[1].copy()\n",
    "\n",
    "                # Filter out pre-market and after market\n",
    "                sample1 = sample.loc[(sample['hour_minute'] < \"16:00:00\") & (sample['hour_minute'] >= \"09:30:00\")]\n",
    "\n",
    "                # Get List of times that should be in the period\n",
    "                given_time = datetime.strptime(doc[0], '%Y-%m-%d')\n",
    "                start_time = given_time + timedelta(hours=9)\n",
    "                start_time = start_time + timedelta(minutes=30)\n",
    "\n",
    "                required_times = []\n",
    "                time = start_time\n",
    "                required_times.append(time)\n",
    "\n",
    "                for x in range(389):\n",
    "                    time = time + timedelta(minutes=1)\n",
    "                    required_times.append(time)\n",
    "\n",
    "                # Make List of times not in period that should be\n",
    "                datetimes_to_add = []\n",
    "\n",
    "                for required_time in required_times:\n",
    "                    if required_time not in list(sample1['datetime']):\n",
    "                        datetimes_to_add.append(required_time)\n",
    "\n",
    "                # Creat variables for the row that should be there\n",
    "                for date in datetimes_to_add:\n",
    "                    time = date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    op = np.nan\n",
    "                    high = np.nan\n",
    "                    low = np.nan\n",
    "                    close = np.nan\n",
    "                    volume = np.nan\n",
    "                    d_time = date\n",
    "                    short_date = date.strftime('%Y-%m-%d')\n",
    "                    hour_min = date.strftime('%H:%M:%S')\n",
    "                    hr = date.hour\n",
    "                    minute = date.minute\n",
    "                    weekday = date.weekday()\n",
    "\n",
    "                    # Create new df (1 row) out of variables\n",
    "                    df_new_line = pd.DataFrame([[time,op,high,low,close,volume,d_time,short_date,hour_min,hr,minute,weekday]], columns=sample1.columns )\n",
    "\n",
    "                    # add the row to the existing df\n",
    "                    sample1 = pd.concat([sample1,df_new_line], ignore_index=True)\n",
    "\n",
    "                # Sort the DF by time\n",
    "                df_to_save = sample1.sort_values(by=['datetime'], ignore_index=True)\n",
    "\n",
    "                # missing data stats\n",
    "                number_missing = sum(df_to_save.open.isna())\n",
    "                number_total = df_to_save.shape[0]\n",
    "                missing_pct = round(number_missing/number_total*100,2)\n",
    "                missing_percents_list.append(missing_pct)\n",
    "                dates_list.append(doc[0])\n",
    "\n",
    "                # Interpolate to fill in missing data.  Try 'forward first' to get previous minute's data, then use future data if that fails.  Shouldn't be too much data\n",
    "                df_to_save = df_to_save.interpolate(method='linear',limit_direction ='forward')\n",
    "                df_to_save = df_to_save.interpolate(method='linear',limit_direction ='backward')\n",
    "\n",
    "                # Add day df into new_combined df\n",
    "                clean_combined = pd.concat([clean_combined,df_to_save],ignore_index=True)     \n",
    "\n",
    "        # Sort the new df by time and delete duplicates\n",
    "        clean_combined.sort_values(\"time\", inplace = True, ignore_index=True) \n",
    "        clean_combined.drop_duplicates(subset =\"time\", keep = 'first', inplace = True, ignore_index=True)\n",
    "\n",
    "        ################################ Save Cleaned Combined CSV ##########################################\n",
    "\n",
    "        print('Saving File')\n",
    "\n",
    "        # See if folder exists, if not, create it\n",
    "        destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '3. Cleaned Combined Data'\n",
    "        if not destination.exists():\n",
    "            destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Set Filename\n",
    "        file_name = minute_interval + \" \" + co + ' cleaned_combined.csv'\n",
    "        file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '3. Cleaned Combined Data' / file_name \n",
    "\n",
    "        # Save File\n",
    "        clean_combined.to_csv(file, index=False)\n",
    "\n",
    "        ################################ Save log of missing percents ########################################\n",
    "        print(\"Logging Missing Data\")\n",
    "\n",
    "        missing_data_df = pd.DataFrame()\n",
    "        missing_data_df['Day'] = dates_list\n",
    "        missing_data_df['%Missing'] = missing_percents_list\n",
    "\n",
    "        # See if folder exists for missing data log, if not, create it\n",
    "        destination = pathlib.Path.cwd() / 'Logs'/ 'Missing Values Logs' / minute_interval\n",
    "        if not destination.exists():\n",
    "            destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        file_name = minute_interval + \" \" + co + ' Missing Values Log.csv'\n",
    "        file = pathlib.Path.cwd() / 'Logs' / 'Missing Values Logs' / minute_interval / file_name\n",
    "\n",
    "        # Save File\n",
    "        missing_data_df.to_csv(file, index=False)\n",
    "    \n",
    "    else:\n",
    "        return\n",
    "\n",
    "    return\n",
    "\n",
    "# Clean the Data\n",
    "for co in tqdm(CoList):\n",
    "    data_cleaner(minute_interval, co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Feature Engineering First Pass\n",
    "\n",
    "Calculates technical indicators for each ETF/Proxy and adds them to the combined CSV and saves a new CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to Calculate Technical Indicators\n",
    "\n",
    "- Bollinger Bands: https://www.investopedia.com/terms/b/bollingerbands.asp\n",
    "- Relative Strength Index (RSI): https://www.investopedia.com/terms/r/rsi.asp\n",
    "- Moving Average Convergence Divergence (MACD): https://www.investopedia.com/terms/m/macd.asp\n",
    "\n",
    "\n",
    "Reference: https://github.com/kconstable/market_predictions/blob/main/market_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bollinger(df,feature,window=20*60,st=2):\n",
    "    \"\"\"\n",
    "    Calculates bollinger bands for a price time-series.  \n",
    "    Input: \n",
    "    df     : A dataframe of time-series prices\n",
    "    feature: The name of the feature in the df to calculate the bands for\n",
    "    window : The size of the rolling window.  Defaults to 20 days with is standard\n",
    "    st     : The number of standard deviations to use in the calculation. 2 is standard \n",
    "    Output: \n",
    "    Returns the df with the bollinger band columns added\n",
    "    \"\"\"\n",
    "\n",
    "    # rolling mean and stdev\n",
    "    rolling_m  = df[feature].rolling(window).mean()\n",
    "    rolling_st = df[feature].rolling(window).std()\n",
    "\n",
    "    # add the upper/lower and middle bollinger bands\n",
    "    df['b-upper']  = rolling_m + (rolling_st * st)\n",
    "    df['b-middle'] = rolling_m \n",
    "    df['b-lower']  = rolling_m - (rolling_st * st)\n",
    "    \n",
    "def calc_rsi(df,feature='close',window=14*60):\n",
    "    \"\"\"\n",
    "    Calculates the RSI for the input feature\n",
    "    Input:\n",
    "    df      : A dataframe with a time-series of prices\n",
    "    feature : The name of the feature in the df to calculate the bands for\n",
    "    window  : The size of the rolling window.  Defaults to 14 days which is standard\n",
    "    Output: \n",
    "    Returns the df with the rsi band column added\n",
    "    \"\"\"\n",
    "    # RSI\n",
    "    # calc the diff in daily prices, exclude nan\n",
    "    diff =df[feature].diff()\n",
    "    diff.dropna(how='any',inplace=True)\n",
    "\n",
    "    # separate positive and negitive changes\n",
    "    pos_m, neg_m = diff.copy(),diff.copy()\n",
    "    pos_m[pos_m<0]=0\n",
    "    neg_m[neg_m>0]=0\n",
    "\n",
    "    # positive/negative rolling means\n",
    "    prm = pos_m.rolling(window).mean()\n",
    "    nrm = neg_m.abs().rolling(window).mean()\n",
    "\n",
    "    # calc the rsi and add to the df\n",
    "    ratio = prm /nrm\n",
    "    rsi = 100.0 - (100.0 / (1.0 + ratio))\n",
    "    df['rsi']=rsi\n",
    "\n",
    "def calc_macd(df,feature='close'):\n",
    "    \"\"\"\n",
    "    Calculates the MACD and signial for the input feature\n",
    "    Input:\n",
    "    df      : A dataframe with a time-series of prices\n",
    "    feature : The name of the feature in the df to calculate the bands for\n",
    "    Output: \n",
    "    Returns the df with the macd columns added\n",
    "    \"\"\"\n",
    "    ema12 = df[feature].ewm(span=12*60,adjust=False).mean()\n",
    "    ema26 = df[feature].ewm(span=26*60,adjust=False).mean()\n",
    "    df['macd']=ema12-ema26\n",
    "    df['macd_signal'] = df['macd'].ewm(span=9*60,adjust=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Technical Indicators to ETF/Proxy Combined CSVs\n",
    "\n",
    "Takes the pricing data and adds technical indicators as features, as well as seasonality features for time of day and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:00<00:00, 441.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_tech_indicators(minute_intervale, co):\n",
    "    \n",
    "    # Set Filename\n",
    "    file_name = minute_interval + \" \" + co + ' cleaned_combined_with_features.csv'\n",
    "    file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / file_name \n",
    "\n",
    "    if not file.is_file():\n",
    "    \n",
    "        ######################################### Open CSV #######################################################\n",
    "        #print(\"Opening File\")\n",
    "        file_name = minute_interval + \" \" + co + ' cleaned_combined.csv'\n",
    "        file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '3. Cleaned Combined Data' / file_name \n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        ##################################### Calculate/Add Technical Indicators #################################\n",
    "\n",
    "        #print('Calculating Technical Indicators')\n",
    "        calc_bollinger(df,'close',window=20,st=2)\n",
    "        calc_rsi(df,feature='close',window=14)\n",
    "        calc_macd(df,feature='close')\n",
    "\n",
    "        ######################################### Seasonality Features ###########################################\n",
    "\n",
    "        date_time = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S')\n",
    "        timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "\n",
    "        day = 24*60*60\n",
    "        year = (365.2425)*day\n",
    "\n",
    "        df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "        df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "        df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "        df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "        #################################### Trim First Day Off because of NaNs #################################\n",
    "        # Technical Indicators calculations use the early data, so the early data won't have technical indicators. \n",
    "        # Trim off first day for easy solution to get rid of NaNs.\n",
    "\n",
    "        df = df[390:]\n",
    "\n",
    "        ################################ Save CSV ##########################################\n",
    "\n",
    "        # See if folder exists, if not, create it\n",
    "        destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features'\n",
    "        if not destination.exists():\n",
    "            destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save File\n",
    "        df.to_csv(file, index=False)\n",
    "    \n",
    "    else:\n",
    "        return\n",
    "\n",
    "    return\n",
    "\n",
    "# Calculate/add technical indicators to each ETF/Proxy\n",
    "for co in tqdm(CoList):\n",
    "    add_tech_indicators(minute_interval, co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Proxy Feature Engineering\n",
    "\n",
    "For each ETF CSV, add the proxy features including:\n",
    "- Close Price\n",
    "- All Technical Indicators\n",
    "\n",
    "Then save dataframe as new CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [26:58<00:00, 29.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load in Proxy DFs\n",
    "SPY = pd.read_csv(pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / '1min SPY cleaned_combined_with_features.csv')\n",
    "TLT = pd.read_csv(pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / '1min TLT cleaned_combined_with_features.csv')\n",
    "VXX = pd.read_csv(pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / '1min VXX cleaned_combined_with_features.csv')\n",
    "XLY = pd.read_csv(pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / '1min XLY cleaned_combined_with_features.csv')\n",
    "VNQ = pd.read_csv(pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / '1min VNQ cleaned_combined_with_features.csv')\n",
    "\n",
    "# Trims Proxies and Renames Columns.  Prepares for merging them with ETF Df\n",
    "proxy_names = ['SPY', 'TLT', 'VXX', 'XLY', 'VNQ']\n",
    "proxies = [SPY, TLT, VXX, XLY, VNQ]\n",
    "trimmed_proxies = []\n",
    "\n",
    "for x in range(len(proxies)):\n",
    "    # Drop redundant columns\n",
    "    proxy = proxies[x].drop(['time','open','high','low','volume','short_date',\n",
    "                             'hour_minute','hour','minute','weekday',\n",
    "                             'Day sin', 'Day cos', 'Year sin', 'Year cos'\n",
    "                            ], axis=1)\n",
    "    \n",
    "    cols = proxy.columns\n",
    "    # Rename unique columns\n",
    "    new_cols = [proxy_names[x] + col if col != 'datetime' else col for col in cols]\n",
    "    proxy.columns = new_cols\n",
    "    # Add proxy to new list\n",
    "    trimmed_proxies.append(proxy)\n",
    "\n",
    "for co in tqdm(CoList):\n",
    "    # Load the ETF df\n",
    "    file_name = '1min '+co+' cleaned_combined_with_features.csv'\n",
    "    etf = pd.read_csv(pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '4. Cleaned Combined Data With Features' / file_name)\n",
    "        \n",
    "    # Combine Proxies \n",
    "    for proxy in trimmed_proxies:\n",
    "        etf = pd.merge(etf, proxy,  how='left', on = ['datetime'])\n",
    "\n",
    "    # Interpolate Missing Values                 \n",
    "    etf = etf.interpolate(method='linear',limit_direction ='forward')\n",
    "    etf = etf.interpolate(method='linear',limit_direction ='backward',limit=5)\n",
    "    \n",
    "    # Save the DF\n",
    "    \n",
    "    # See if folder exists, if not, create it\n",
    "    destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '5. Cleaned Combined Data With Proxies'\n",
    "    if not destination.exists():\n",
    "        destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set Filename\n",
    "    file_name = minute_interval + \" \" + co + ' cleaned_combined_with_proxies.csv'\n",
    "    file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '5. Cleaned Combined Data With Proxies' / file_name \n",
    "\n",
    "    if not file.is_file():\n",
    "        # Save File\n",
    "        etf.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Data Stationarity Check\n",
    "\n",
    "Deep learning performs better on stationary data.  But as the EDA has shown, pricing data for these ETFs are not usually stationary.  The stock price fluctuates over time and has clear trends over time.\n",
    "\n",
    "References: \n",
    "\n",
    "https://analyzingalpha.com/check-time-series-stationarity-python#augmented-dickey-fuller-adf\n",
    "https://github.com/kconstable/market_predictions/blob/main/LSTM_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_stationary(df,features_to_transform,transform='log'):\n",
    "    \"\"\"\n",
    "    Transform time-series data using a log or boxcox transform.  Calculate the augmented\n",
    "    dickey-fuller (ADF) test for stationarity after the transform\n",
    "    Inputs:\n",
    "    df: a dataframe of features\n",
    "    features_to_transform: A list of features to apply the transform\n",
    "    transform: The transform to apply (log, boxbox)\n",
    "    Output\n",
    "    Applies the transforms inplace in df\n",
    "    \"\"\"\n",
    "    # transform each column in the features_to_transform list\n",
    "    for feature in df.columns:\n",
    "        if feature in features_to_transform:\n",
    "            # log transform\n",
    "            if transform=='log':\n",
    "                df[feature] = df[feature].apply(np.log)\n",
    "\n",
    "            # boxcox transform  \n",
    "            elif transform=='boxcox':\n",
    "                bc,_ = stats.boxcox(df[feature])\n",
    "                df[feature] = bc\n",
    "\n",
    "            else:\n",
    "                print(\"Transformation not recognized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the ETF pricing data and makes it stationary, and saves a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv_stationary(co, features_to_transform):\n",
    "    file_name = minute_interval + \" \" + co + ' cleaned_combined_stationary.csv'\n",
    "    file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '6. Cleaned Combined Data Stationarity' / file_name \n",
    "    \n",
    "    if not file.is_file():\n",
    "    \n",
    "        # Read in File to edit\n",
    "        file_name = minute_interval + \" \" + co + ' cleaned_combined_with_proxies.csv'\n",
    "        file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '5. Cleaned Combined Data With Proxies' / file_name \n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        close_prices = df.copy()['close']\n",
    "        transform_stationary(df,features_to_transform,'log')\n",
    "\n",
    "        df['close_prices'] = close_prices\n",
    "\n",
    "        # See if folder exists, if not, create it\n",
    "        destination = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '6. Cleaned Combined Data Stationarity'\n",
    "        if not destination.exists():\n",
    "            destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Set Filename\n",
    "        file_name = minute_interval + \" \" + co + ' cleaned_combined_stationary.csv'\n",
    "        file = pathlib.Path.cwd() / 'AlphaVantage Data'/ minute_interval / '6. Cleaned Combined Data Stationarity' / file_name \n",
    "        df.to_csv(file, index=False)\n",
    "        \n",
    "        return\n",
    "        \n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Function for Each ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [25:43<00:00, 28.58s/it]\n"
     ]
    }
   ],
   "source": [
    "features_to_transform = ['open', 'high', 'low', 'close','b-upper','b-middle', 'b-lower',\n",
    "    'SPYclose','SPYb-upper', 'SPYb-middle', 'SPYb-lower', \n",
    "    'TLTclose', 'TLTb-upper', 'TLTb-middle', 'TLTb-lower',\n",
    "    'VXXclose', 'VXXb-upper', 'VXXb-middle', 'VXXb-lower', \n",
    "    'XLYclose', 'XLYb-upper', 'XLYb-middle', 'XLYb-lower', \n",
    "    'VNQclose', 'VNQb-upper', 'VNQb-middle','VNQb-lower',]\n",
    "\n",
    "for co in tqdm(CoList):\n",
    "    make_csv_stationary(co, features_to_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is downloaded, feature engineered, cleaned, and made stationary. The next notebook will prepare it for deep learning and then train the models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. Import Packages\n",
    "\n",
    "2. Define Functions\n",
    "\n",
    "3. Run Functions to Train Models, Save Models, and Log Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras import models, layers, backend, regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, actual_close_prices, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Generates batches of data for the model to train on.  Saves space \n",
    "    in memory, which allows Deep Learning models to analyze big datasets.\n",
    "    \n",
    "    Generators will be run on the train and validation data.\n",
    "    \n",
    "    Inputs cleaned pricing data and outputs the generator.\n",
    "    \n",
    "    Adapted from: Deep Learning With Python, by Francis Chollet\n",
    "    ###################################################################\n",
    "\n",
    "    '''\n",
    "    # Checks index\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    \n",
    "    # Gets samples and targets for each item in the batch\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        \n",
    "        for j, row in enumerate(rows[:-2]):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            #targets[j] = data[rows[j] + delay][1]\n",
    "            \n",
    "            # Calculate custom target\n",
    "            beg = actual_close_prices[rows[j]]\n",
    "            end = actual_close_prices[rows[j] + delay]\n",
    "            value = (end-beg)/beg\n",
    "            \n",
    "            if value > 0:\n",
    "                targets[j] = 1\n",
    "            else:\n",
    "                targets[j] = 0\n",
    "                \n",
    "        yield samples, targets\n",
    "        \n",
    "def train_validation_test_split_and_scaling(float_data, train_percent, val_percent):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Gets the row numbers for train, validation, and test sets within the \n",
    "    array.  Scales the data in place based on the test data.\n",
    "    \n",
    "    Input is array of data with split percents.\n",
    "    \n",
    "    Output is the scaled data and the data point end rows.\n",
    "    ###################################################################\n",
    "    '''\n",
    "    \n",
    "    # Train, validation, test split\n",
    "    n = len(float_data)\n",
    "\n",
    "    train_data_end = int(n*train_percent) #0.7\n",
    "    val_data_end = int(n* (train_percent + val_percent)) #0.9\n",
    "    test_data_end = n\n",
    "\n",
    "    # Feature Scaling\n",
    "    mean = float_data[:train_data_end].mean(axis=0)\n",
    "    float_data -= mean\n",
    "    std = float_data[:train_data_end].std(axis=0)\n",
    "    float_data /= std\n",
    "    \n",
    "    return float_data, train_data_end, val_data_end,test_data_end\n",
    "\n",
    "def fit_model(train_gen, val_gen, val_steps, test_array, test_labels, verbose):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Creates the LSTM model, compiles it, and trains it.  \n",
    "    \n",
    "    Inputs are the generators and test data and labels.\n",
    "    \n",
    "    Outputs the model and its history\n",
    "    ###################################################################\n",
    "    '''\n",
    "\n",
    "    backend.clear_session()\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.LSTM(128,\n",
    "                         dropout=0.2,\n",
    "                         recurrent_dropout=0.5,\n",
    "                         return_sequences=True,\n",
    "                         input_shape=(None, float_data.shape[-1])))\n",
    "\n",
    "    model.add(layers.LSTM(128, activation='relu',\n",
    "                         dropout=0.2,\n",
    "                         recurrent_dropout=0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=RMSprop(), loss='binary_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(train_gen,\n",
    "                      shuffle=False,\n",
    "                      steps_per_epoch=100,\n",
    "                      epochs=40,\n",
    "                      validation_data=val_gen,\n",
    "                      validation_steps=val_steps,\n",
    "                      verbose = verbose,\n",
    "                      callbacks=[EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights = True)])\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def plot_model_loss_accuracy(history):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Plots Model Evaluation Metrics\n",
    "    ###################################################################\n",
    "    '''\n",
    "\n",
    "    history_dict = history.history\n",
    "    loss_values = history_dict['loss']\n",
    "    val_loss_values = history_dict['val_loss']\n",
    "    acc_values = history_dict['accuracy']\n",
    "    val_acc_values = history_dict['val_accuracy']\n",
    "    epochs = range(1, len(history_dict['accuracy']) + 1)\n",
    "\n",
    "    plt.plot(epochs, loss_values, 'bo', label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epochs, acc_values, 'bo', label = 'Training accuracy')\n",
    "    plt.plot(epochs, val_acc_values, 'b', label = 'Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "def save_model(delay, co, model):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Saves the Model Locally\n",
    "    ###################################################################\n",
    "    '''\n",
    "\n",
    "    str_delay = str(delay) + ' min'\n",
    "    \n",
    "    destination = pathlib.Path.cwd() / 'Models'/ str_delay\n",
    "    if not destination.exists():\n",
    "        destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    path = 'Models/'+ str_delay +'/'\n",
    "    file_name = path + co + ' ' + str_delay + \".h5\"\n",
    "\n",
    "    model.save(file_name, save_format=\"tf\")\n",
    "    \n",
    "    return\n",
    "\n",
    "def load_a_model(delay, co):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Loads in the model from local storage\n",
    "    ###################################################################\n",
    "    \n",
    "    '''\n",
    "    str_delay = str(delay) + ' min'\n",
    "    path = 'Models/'+ str_delay +'/'\n",
    "    file_name = path + co + ' ' + str_delay + \".h5\"\n",
    "\n",
    "    loaded_model = load_model(file_name)\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "def generator_and_label_provider(float_data, actual_close_prices, lookback,step, delay, batch_size, train_data_end,val_data_end):\n",
    "\n",
    "    '''\n",
    "    ###################################################################\n",
    "    A versatile function to prepare data for model training and \n",
    "    evaluation.  Creates the train and validation generators as the\n",
    "    testing data and labels. \n",
    "    ###################################################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Create Generators\n",
    "    train_gen = generator(float_data,actual_close_prices,\n",
    "                          lookback=lookback,\n",
    "                          delay=delay,\n",
    "                          min_index=0,\n",
    "                          max_index=train_data_end,\n",
    "                          shuffle=True,\n",
    "                          step=step,\n",
    "                          batch_size=batch_size)\n",
    "    val_gen = generator(float_data, actual_close_prices,\n",
    "                        lookback=lookback,\n",
    "                        delay=delay,\n",
    "                        min_index=train_data_end,\n",
    "                        max_index=val_data_end,\n",
    "                        step=step,\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    val_steps = (val_data_end - train_data_end - lookback) // step // batch_size\n",
    "\n",
    "    #test_steps = (len(float_data) - val_data_end - lookback) // step // batch_size\n",
    "\n",
    "    # Get test data and test labels\n",
    "    test_data = float_data[val_data_end:]\n",
    "    test_prices = actual_close_prices[val_data_end:]\n",
    "\n",
    "    test_data_list = []\n",
    "    test_values_list = []\n",
    "    test_hours_list = []\n",
    "    test_minutes_list = []\n",
    "    test_weekdays_list = []\n",
    "    \n",
    "    # Saves test data in chunks and labels and misc data for evaluations and result logging\n",
    "    for x in range(lookback, len(test_data) - delay, step):\n",
    "        x_data = test_data[x-lookback:x]\n",
    "        test_data_list.append(x_data)\n",
    "        end_price = test_prices[x+delay]\n",
    "        beg_price = test_prices[x]\n",
    "        value_change = (end_price - beg_price) / beg_price\n",
    "        test_values_list.append(value_change)\n",
    "\n",
    "        test_hours_list.append(hours[x])\n",
    "        test_minutes_list.append(minutes[x])\n",
    "        test_weekdays_list.append(weekdays[x])\n",
    "\n",
    "    test_array = np.asarray(test_data_list)\n",
    "\n",
    "    # Converts percent values to binary labels\n",
    "    test_labels = []\n",
    "\n",
    "    for label in test_values_list:\n",
    "        if label > 0:\n",
    "            test_labels.append(1)\n",
    "        else:\n",
    "            test_labels.append(0)\n",
    "\n",
    "    test_labels = np.asarray(test_labels)\n",
    "\n",
    "    # Counts class instances, for class balance checks\n",
    "    zero_count = 0\n",
    "    one_count = 0\n",
    "\n",
    "    for label in test_labels:\n",
    "        if label == 0:\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            one_count += 1\n",
    "\n",
    "    #print(\"Zeros: \", zero_count)\n",
    "    #print(\"Ones: \", one_count)\n",
    "    \n",
    "    return train_gen, val_gen, val_steps, zero_count, one_count, test_array, test_labels, test_values_list, test_hours_list, test_minutes_list, test_weekdays_list\n",
    "\n",
    "def log_result_by_delay(model, test_array, test_labels, test_values_list, delay):\n",
    "    \n",
    "    '''\n",
    "    ###################################################################\n",
    "    Logs the results by delay period, which is 30 minutes for this project\n",
    "    but different delay periods could be experimented wtih.  For the given\n",
    "    model, the fucntion calculates ROC Score, Accuracy, investment returns,\n",
    "    passive returns, and a classification matrix.  The function ends by\n",
    "    loggin the results into a spreadsheet.\n",
    "    ###################################################################\n",
    "    '''\n",
    "    \n",
    "    ########################### Run Metrics ############################################\n",
    "    test_loss, test_acc = model.evaluate(test_array, test_labels, verbose=0)\n",
    "\n",
    "    y_pred = model.predict(test_array)\n",
    "    y_pred_list = []\n",
    "\n",
    "    # Convert probabilities to binary prediction\n",
    "    for item in y_pred:\n",
    "        if item > .50: #.50\n",
    "            y_pred_list.append(1)\n",
    "        else:\n",
    "            y_pred_list.append(0)\n",
    "\n",
    "    # For companies that the model predicts as postive, add the % return to a list and average\n",
    "    investments = []\n",
    "    for x in range(len(test_values_list)):\n",
    "        if list(y_pred_list)[x] == 1:\n",
    "            investments.append(list(test_values_list)[x])\n",
    "\n",
    "    # Compare average returns given by the model, compared to average returns overall\n",
    "    try:\n",
    "        strat_avg = statistics.mean(investments)\n",
    "    except:\n",
    "        strat_avg = 0\n",
    "\n",
    "    try:\n",
    "        passive_avg = statistics.mean(test_values_list)\n",
    "    except:\n",
    "        passive_avg = 0\n",
    "\n",
    "    '''\n",
    "    ########################################################################################################################\n",
    "    ROC Score\n",
    "    ########################################################################################################################\n",
    "    '''\n",
    "    try:\n",
    "        roc_score = roc_auc_score(test_labels, y_pred)\n",
    "    except:\n",
    "        roc_score = 0\n",
    "        \n",
    "    print('ROC Score:', roc_score)\n",
    "    print('Strategy Returns', round(strat_avg,5)*100,\"%\")\n",
    "    print('Passive Returns:', round(passive_avg,5)*100,\"%\")\n",
    "\n",
    "    '''\n",
    "    ########################################################################################################################\n",
    "    Classification Report\n",
    "    ########################################################################################################################\n",
    "    '''\n",
    "    #print(\"________________________________________________________________________\")\n",
    "    #print(\"Classification Report\")\n",
    "    #print(classification_report(test_labels, y_pred_list, zero_division=0))\n",
    "\n",
    "    '''\n",
    "    ########################################################################################################################\n",
    "    Plot Confusion Matrix\n",
    "    ########################################################################################################################\n",
    "    '''\n",
    "    cm = confusion_matrix(test_labels, y_pred_list)\n",
    "\n",
    "    try:\n",
    "        tp = cm[1][1]\n",
    "        fp = cm[0][1]\n",
    "        tn = cm[0][0]\n",
    "        fn = cm[1][0]\n",
    "    except:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        tn = 0\n",
    "        fn = 0\n",
    "\n",
    "    ######################################## Initiate Log ##################################\n",
    "\n",
    "    # See if folder exists, if not, create it\n",
    "    destination = pathlib.Path.cwd() / 'Logs'/ 'Result Logs' / 'By Delay'\n",
    "    if not destination.exists():\n",
    "        destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # See if file exists, if not create it    \n",
    "    file_name = 'Result Log by Delay - '+str(delay)+' min.csv'\n",
    "    file = pathlib.Path.cwd() / 'Logs'/ 'Result Logs' / 'By Delay' / file_name\n",
    "\n",
    "    if not file.is_file():\n",
    "        cols = ['Ticker', 'Zero_count', 'One_count', 'Accuracy', 'ROC Score', 'TP', 'FP','TN','FN', 'Strat Return', 'Passive Return']\n",
    "\n",
    "        result_log = pd.DataFrame(columns=cols)\n",
    "        result_log.to_csv(file, index=False)\n",
    "\n",
    "    ######################################### Add to File #####################################\n",
    "\n",
    "    #open file\n",
    "    result_log = pd.read_csv(file)\n",
    "\n",
    "    # create new row\n",
    "    cols = ['Ticker', 'Zero_count', 'One_count', 'Accuracy', 'ROC Score', 'TP', 'FP','TN','FN', 'Strat Return', 'Passive Return']\n",
    "\n",
    "    new_row = pd.DataFrame([[co, zero_count, one_count,test_acc,roc_score,tp,fp,tn,fn,strat_avg,passive_avg]], columns=cols)\n",
    "\n",
    "    # append row to file\n",
    "    result_log = result_log.append(new_row, ignore_index=True)\n",
    "\n",
    "    # Save\n",
    "    result_log.to_csv(file, index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "def log_result_by_window(model, delay, test_array, test_labels, test_values_list, test_hours_list, test_minutes_list, test_weekdays_list):\n",
    "\n",
    "    '''\n",
    "    ###################################################################\n",
    "    This function is similar to the one above except it logs results on\n",
    "    a more granular basis.  This function breaks the test data into \n",
    "    trading windows.  The trading week is broken down into Day, hour, \n",
    "    minute (0 or 30), windows.  So each week has 70 windows.  The test \n",
    "    data represents several weeks, so this function evaluates model \n",
    "    performance on a granular basis by averaging or calculating metrics\n",
    "    on a subset of the data - each window.\n",
    "    ###################################################################\n",
    "    '''\n",
    "    \n",
    "    ######################################## Initiate Log ##################################\n",
    "\n",
    "    # See if folder exists, if not, create it\n",
    "    destination = pathlib.Path.cwd() / 'Logs'/ 'Result Logs' / 'By Window'\n",
    "    if not destination.exists():\n",
    "        destination.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    # See if file exists, if not create it    \n",
    "    file_name = 'Result Log by Window - '+str(delay)+' min.csv'\n",
    "    file = pathlib.Path.cwd() / 'Logs'/ 'Result Logs' / 'By Window' / file_name\n",
    "\n",
    "    if not file.is_file():\n",
    "        cols = ['Ticker', 'Weekday','Hour','Minute', 'ROC Score', 'TP', 'FP','TN','FN', 'Strat Return', 'Passive Return']\n",
    "\n",
    "        result_log = pd.DataFrame(columns=cols)\n",
    "        result_log.to_csv(file, index=False)\n",
    "\n",
    "    ######################################### Add to File #####################################\n",
    "\n",
    "    #open file\n",
    "    result_log = pd.read_csv(file)\n",
    "\n",
    "    ######################################## Create Result DF ##################################\n",
    "    pred_df = pd.DataFrame()\n",
    "\n",
    "    y_pred = model.predict(test_array)\n",
    "    y_pred_list = [x[0] for x in y_pred]\n",
    "\n",
    "    pred_df['pred'] = y_pred_list\n",
    "    pred_label = [1 if x >.50 else 0 for x in y_pred_list]\n",
    "    pred_df['predicted label'] = pred_label\n",
    "    pred_df['actual label'] = test_labels\n",
    "    pred_df['% return'] = test_values_list\n",
    "    pred_df['hour'] = test_hours_list\n",
    "    pred_df['minute'] = test_minutes_list\n",
    "    pred_df['weekday'] = test_weekdays_list\n",
    "\n",
    "    ###################################### Group DF by each Period ##################################\n",
    "\n",
    "    for group in pred_df.groupby(['hour','minute','weekday']):\n",
    "\n",
    "        hour = group[0][0]\n",
    "        minute = group[0][1]\n",
    "        weekday = group[0][2]\n",
    "\n",
    "        try:\n",
    "            roc_score = roc_auc_score(group[1]['actual label'], group[1]['predicted label'])\n",
    "        except:\n",
    "            roc_score = 0\n",
    "\n",
    "        cm = confusion_matrix(group[1]['actual label'], group[1]['predicted label'])\n",
    "\n",
    "        try:\n",
    "            tp = cm[1][1]\n",
    "            fp = cm[0][1]\n",
    "            tn = cm[0][0]\n",
    "            fn = cm[1][0]\n",
    "        except:\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            tn = 0\n",
    "            fn = 0\n",
    "            \n",
    "\n",
    "        #print(len(group[1]['predicted label']))\n",
    "\n",
    "        # For companies that the model predicts as postive, add the % return to a list and average\n",
    "        investments = []\n",
    "        for x in range(len(group[1]['% return'])):\n",
    "            if list(group[1]['predicted label'])[x] == 1:\n",
    "                investments.append(list(group[1]['% return'])[x])\n",
    "\n",
    "        # Compare average returns given by the model, compared to average returns overall\n",
    "        try:\n",
    "            strat_avg = statistics.mean(investments)\n",
    "        except:\n",
    "            strat_avg = 0\n",
    "\n",
    "        try:\n",
    "            passive_avg = statistics.mean(group[1]['% return'])\n",
    "        except:\n",
    "            passive_avg = 0\n",
    "\n",
    "        strat_returns = strat_avg\n",
    "        passive_returns = passive_avg\n",
    "\n",
    "        # create new row\n",
    "        cols = ['Ticker', 'Weekday','Hour','Minute', 'ROC Score', 'TP', 'FP','TN','FN', 'Strat Return', 'Passive Return']\n",
    "\n",
    "        new_row = pd.DataFrame([[co,weekday, hour, minute ,roc_score,tp,fp,tn,fn,strat_avg,passive_avg]], columns=cols)\n",
    "\n",
    "        # append row to file\n",
    "        result_log = result_log.append(new_row, ignore_index=True)\n",
    "\n",
    "\n",
    "    # Save\n",
    "    result_log.to_csv(file, index=False)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Functions to Train Models, Save Models, and Log Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 Mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: AGG\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: AMLP\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: BND\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: DIA\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: EEM\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: EFA\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: EWJ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: EWZ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: FXI\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: GDX\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: GDXJ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: GLD\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: HYG\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IAU\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IEFA\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IEMG\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IJR\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IVV\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IWM\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: IYR\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: JDST\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: KRE\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: LQD\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: NUGT\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: PFF\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: QQQ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: SLV\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: SPXL\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: SPXU\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: SPY\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: SQQQ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: TLT\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: TQQQ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: TZA\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: UNG\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: UPRO\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: USO\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: UVXY\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: VEA\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: VNQ\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: VTI\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: VWO\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: VXX\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLB\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLC\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLE\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLF\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLK\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLP\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLU\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLV\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XLY\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XOP\n",
      "+++++++++++++++++++++++++++++++++++++++\n",
      "Processing: XRT\n",
      "All Finished\n",
      "Cos added: 0\n",
      "Cos Already Existing: 54\n",
      "Total Cos in Folders: 54\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################\n",
    "# Import List of Stocks and Proxies\n",
    "###############################################################################################\n",
    "\n",
    "# List of Stocks\n",
    "#CoList = pd.read_excel('Input Files/List of ETFs.xlsx',sheet_name='Low_Missing')\n",
    "CoList = pd.read_excel('Input Files/List of ETFs.xlsx',sheet_name='Demo')\n",
    "\n",
    "CoList = CoList['Symbol']\n",
    "\n",
    "# List of Proxies\n",
    "proxies = pd.read_excel('Input Files/List of ETFs.xlsx',sheet_name='Proxies')\n",
    "\n",
    "proxies = proxies['Symbol']\n",
    "\n",
    "# List of Market Holidays\n",
    "holiday_list = list(pd.read_excel('Input Files/Stock Market Holidays.xlsx')['Date'])\n",
    "holiday_list = [holiday.strftime('%Y-%m-%d') for holiday in holiday_list]\n",
    "\n",
    "################## Set Interval #######################\n",
    "# 1min, 5min, 15min, 30min, 60min\n",
    "minute_interval = '1min'\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################################\n",
    "# Set Parameters\n",
    "###############################################################################################\n",
    "delay = 30\n",
    "\n",
    "lookback = 390*3\n",
    "step = 30\n",
    "batch_size = 128\n",
    "\n",
    "###############################################################################################\n",
    "# Run Functions to Train all the models, save them, and log the results\n",
    "###############################################################################################\n",
    "cos_added = 0\n",
    "cos_existing = 0\n",
    "\n",
    "for co in CoList:\n",
    "    \n",
    "    print('+++++++++++++++++++++++++++++++++++++++')\n",
    "    print('Processing:',co)\n",
    "    \n",
    "    # Make sure model doesn't already exist, skip if so\n",
    "    str_delay = str(delay) + ' min'\n",
    "    path = 'Models/'+ str_delay +'/'\n",
    "    file = path + co + ' ' + str_delay + \".h5\"\n",
    "\n",
    "    if not exists(file):\n",
    "        \n",
    "        ######################### Load in the dataset #############################################\n",
    "        \n",
    "        file = 'AlphaVantage Data/1min/6. Cleaned Combined Data Stationarity/1min '+co+' cleaned_combined_stationary.csv'\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        df = df.drop(columns=['time','datetime','short_date', 'hour_minute'])\n",
    "\n",
    "        hours = df['hour']\n",
    "        minutes = df['minute']\n",
    "        weekdays = df['weekday']\n",
    "\n",
    "        actual_close_prices = np.asarray(df['close_prices'])\n",
    "        float_data = df.to_numpy()\n",
    "        \n",
    "        ################################## Split into Train, Validation, and Test data, and scale features ##################\n",
    "        # 65% train data, 15% validation data, 20% Test Data\n",
    "        float_data, train_data_end, val_data_end,test_data_end = train_validation_test_split_and_scaling(float_data, .70, .20)\n",
    "        \n",
    "        ############################# Create Data Generators and Make Labels and Testing Aids ######################\n",
    "        train_gen, val_gen, val_steps, zero_count, one_count, test_array, test_labels, test_values_list, test_hours_list, test_minutes_list, test_weekdays_list =  generator_and_label_provider(float_data, actual_close_prices, lookback,step, delay, batch_size, train_data_end,val_data_end)\n",
    "        \n",
    "        print(\"Training the Model\")\n",
    "        ####################################### Fit the Model #############################################\n",
    "        \n",
    "        # Enables multiple tries\n",
    "        third_met = False\n",
    "        third_counter = 0\n",
    "        \n",
    "        while third_met == False:\n",
    "            \n",
    "            model, history = fit_model(train_gen, val_gen, val_steps, test_array, test_labels, 0)\n",
    "            \n",
    "            y_pred = model.predict(test_array)\n",
    "            y_pred_list = []\n",
    "\n",
    "            # Convert probabilities to binary prediction\n",
    "            for item in y_pred:\n",
    "                if item > .50: #.50\n",
    "                    y_pred_list.append(1)\n",
    "                else:\n",
    "                    y_pred_list.append(0)\n",
    "            \n",
    "            cm = confusion_matrix(test_labels, y_pred_list)\n",
    "            try:\n",
    "                tp = cm[1][1]\n",
    "                fp = cm[0][1]\n",
    "                tn = cm[0][0]\n",
    "                fn = cm[1][0]\n",
    "            except:\n",
    "                tp = 0\n",
    "                fp = 0\n",
    "                tn = 0\n",
    "                fn = 0\n",
    "                \n",
    "            # Try multiple times unless model makes semi-balanced prediction, otherwise some models predict < 1% for some classes\n",
    "            if (tp+fp)/(tp+fp+tn+fn) > .30 and (tn+fn)/(tp+fn+tn+fn) > .30:\n",
    "                third_met = True\n",
    "                print(\"Third Met\")\n",
    "                \n",
    "            elif third_counter > 3:\n",
    "                third_met = True\n",
    "                print(\"Model Tried More than three times...advancing\")\n",
    "            else:\n",
    "                third_counter += 1\n",
    "                print(\"Third Not Met\")\n",
    "        \n",
    "\n",
    "        ################################ Save the Model and Load it back in for testing ######################\n",
    "        save_model(delay, co, model)\n",
    "        model = load_a_model(delay, co)\n",
    "        \n",
    "        print(\"Logging Result\")\n",
    "        ################################## Log the Result by each delay period ###############################\n",
    "        log_result_by_delay(model,test_array, test_labels, test_values_list, delay)\n",
    "        \n",
    "        ################################## Log Result by each trading window for each delay period ####################\n",
    "        log_result_by_window(model, delay, test_array, test_labels, test_values_list, test_hours_list, test_minutes_list, test_weekdays_list)\n",
    "        \n",
    "        cos_added += 1\n",
    "        \n",
    "    else:\n",
    "        cos_existing += 1\n",
    "        continue\n",
    "        \n",
    "print('All Finished')\n",
    "print('Cos added:', cos_added)\n",
    "print('Cos Already Existing:', cos_existing)\n",
    "print('Total Cos in Folders:',cos_added + cos_existing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
